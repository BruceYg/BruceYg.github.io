## About Me
I am a first-year PhD student in Engineering at the University of Cambridge, supervised by [Prof. Bill Byrne](https://sites.google.com/view/bill-byrne/home). I am fully funded by the Cambridge International Scholarship from Cambridge Trust. Prior to this, I worked as a Machine Learning Engineer at To0 Space, a start-up focused on generating architectural rendering images using diffusion models.

I hold an M.Phil degree in Machine Learning and Machine Intelligence from the University of Cambridge, where I specialized in speech and language processing and supervised by Prof. Byrne. I obtained my B.S degree from the University of California, San Diego with double majors in Computer Science and Cognitive Science, where I was a member of the Caledonian Honor Society at Muir College and graduated Cum Laude.

My research interests include Multilingual Models, Neural Machine Translation, LLM Alignment, Retrieval Augmented Generation, and Minimum Bayes Risk Decoding.

## Education
- **Ph.D**, Engineering,  University of Cambridge (_Oct 2024 - Present_)
- **M.Phil**, Machine Learning and Machine Intelligence,  University of Cambridge (_Oct 2022_-_Aug 2023_)
- **B.S**, Computer Science/Cognitive Science,  UC San Diego (_Oct 2018_-_Jun 2022_), *Cum Laude*

## Publications
1. Jinghong Chen, **Guangyu Yang**, Weizhe Lin, Jingbiao Mei, and Bill Byrne. [On Extending Direct Preference Optimization to Accommodate Ties](https://arxiv.org/abs/2409.17431) (ArXiv Preprint).
2. **Guangyu Yang**, Jinghong Chen, Weizhe Lin, and Bill Byrne. [Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding](https://aclanthology.org/2024.naacl-short.34/) (NAACL 2024).
3. **Guangyu Yang**. [Multilingual Models in Neural Machine Translation](https://www.mlmi.eng.cam.ac.uk/files/2022_-_2023_dissertations/multilingual_models_in_neural_machine_translation.pdf) (MPhil Thesis).

## Working Experience
**Machine Learning Engineer @ To0 Space (_Sep 2023 - Sep 2024_)**
- Fine-tuning Stable Diffusion (SD) model and Vision Transformer (ViT) for architecture rendering image.
- Retrieval Augmented Generation (RAG) for architecture rendering.
- Automatic labelling and captioning for architecture images with Vision Language Models (VLMs) and Large Language Models (LLMs).

## Teaching Experience
**Instructional Assistant @ UC San Diego**
- COGS 118A. Supervised Machine Learning Algorithms
